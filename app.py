"""
üóëÔ∏è clean_the_garbage.exe ‚Äî Resume Ranker (Ultimate v5+)
Company: Lola, Liza & Partners LLC

–§—É–Ω–∫—Ü–∏–∏:
- LLM (OpenAI, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é gpt-4o-mini) –∏–∑–≤–ª–µ–∫–∞–µ—Ç –§–ò–û, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é (+–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã), –∫–æ–Ω—Ç–∞–∫—Ç—ã,
  –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º 0‚Äì5 –∏ –¥–∞—ë—Ç –∫—Ä–∞—Ç–∫–∏–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è ‚Äî –í–°–Å –∑–∞ –æ–¥–∏–Ω –±–∞—Ç—á-–∑–∞–ø—Ä–æ—Å (–¥–æ 5 —Ä–µ–∑—é–º–µ).
- –£—Å—Ç–æ–π—á–∏–≤—ã–π –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π —Å–∫–æ—Ä–∏–Ω–≥: 0.75*wP(–ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª–∏) + 0.25*Coverage.
- –ñ—ë—Å—Ç–∫–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è: –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ñ–∞–π–ª—ã/—Ç–µ–∫—Å—Ç—ã, –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ email/—Ç–µ–ª–µ—Ñ–æ–Ω—ã, Similarity >= –ø–æ—Ä–æ–≥–∞ (100% ‚Äî –≤—Å–µ–≥–¥–∞ –¥—É–±–ª–∏–∫–∞—Ç).
- –í—ã–≥—Ä—É–∑–∫–∞ –¢–û–õ–¨–ö–û –≤ XLSX. –ë–æ—Ä–¥–µ—Ä—ã, –∂–∏—Ä–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏, —à–∫–∞–ª—ã, –ø–æ–¥—Å–≤–µ—Ç–∫–∞ —Å—Ç—Ä–æ–∫ –ø–æ –±–∞–∫–µ—Ç–∞–º/—Ä–∏—Å–∫–∞–º.
- –ß–µ–∫–ø–æ–∏–Ω—Ç JSONL –ø–æ sha1 –æ—Ç –±–∞–π—Ç–æ–≤ ‚Äî –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.

–£—Å—Ç–∞–Ω–æ–≤–∫–∞:
    pip install streamlit pdfminer.six python-docx rapidfuzz pandas openpyxl pydantic tenacity openai

–ó–∞–ø—É—Å–∫:
    streamlit run app.py
"""

from __future__ import annotations
import io, os, re, json, hashlib
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
import streamlit as st
from pydantic import BaseModel, Field
from rapidfuzz import fuzz

# ---------- Parsers ----------
try:
    from pdfminer.high_level import extract_text as pdf_extract_text
except Exception:
    pdf_extract_text = None
try:
    import docx
except Exception:
    docx = None

ALLOWED_EXT = {".pdf", ".docx", ".txt", ".md", ".rtf"}
TOKEN_SPLIT = re.compile(r"[\W_]+", re.UNICODE)
EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
PHONE_CAND_RE = re.compile(r"(?:\+?\d[\d\-\s()\./]{6,}\d)")

# ---------- Models ----------
class Criterion(BaseModel):
    name: str
    weight: float = Field(1.0, ge=0)
    keywords: List[str] = Field(default_factory=list)

class LLMScores(BaseModel):
    scores: Dict[str, float]
    reasoning: Dict[str, str] = Field(default_factory=dict)

# ---------- Utils ----------
def read_file_text(filename: str, bytes_data: bytes) -> str:
    ext = os.path.splitext(filename)[1].lower()
    if ext == ".pdf":
        if not pdf_extract_text:
            raise RuntimeError("pdfminer.six –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. pip install pdfminer.six")
        with io.BytesIO(bytes_data) as bio:
            return pdf_extract_text(bio)
    if ext == ".docx":
        if not docx:
            raise RuntimeError("python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. pip install python-docx")
        with io.BytesIO(bytes_data) as bio:
            d = docx.Document(bio)
            return "\n".join(p.text for p in d.paragraphs)
    if ext in {".txt", ".md", ".rtf"}:
        try:
            return bytes_data.decode("utf-8", errors="ignore")
        except Exception:
            return bytes_data.decode("latin-1", errors="ignore")
    raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {ext}")

def sha1_bytes(b: bytes) -> str:
    return hashlib.sha1(b).hexdigest()

def sha1_text(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()

def normalize_for_sim(t: str) -> str:
    t = re.sub(r"\s+", " ", t.lower())
    return re.sub(r"[^a-z–∞-—è0-9 ]+", " ", t)

def normalize_phone_digits(s: str) -> str:
    return "".join(ch for ch in s if ch.isdigit() or ch == "+")

def best_phone(phones: List[str]) -> str:
    cleaned = []
    for p in phones:
        norm = normalize_phone_digits(p)
        digits = "".join(d for d in norm if d.isdigit())
        if 10 <= len(digits) <= 15:
            cleaned.append(norm)
    if not cleaned:
        return ""
    cleaned.sort(key=lambda x: (not x.startswith("+"), -len(x)))
    return cleaned[0]

# --- —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –§–ò–û (—Ñ–æ–ª–±—ç–∫) ---
FIO_RE = re.compile(r"\b[–ê-–Ø–Å][–∞-—è—ë]+ [–ê-–Ø–Å][–∞-—è—ë]+(?: [–ê-–Ø–Å][–∞-—è—ë]+)?\b")

def guess_fio(text: str) -> str:
    lines = [l.strip() for l in text.splitlines() if l.strip()][:30]
    for l in lines:
        if EMAIL_RE.search(l) or PHONE_CAND_RE.search(l) or "http" in l.lower():
            continue
        m = FIO_RE.search(l)
        if m:
            return m.group(0)
    return ""

def chunks(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i+n]

def clamp(s: str, max_len: int) -> str:
    s = re.sub(r"\s+", " ", (s or "").strip())
    return (s[:max_len-1] + "‚Ä¶") if len(s) > max_len else s

# ---------- LLM ----------
class OpenAIClientWrapper:
    def __init__(self, api_key: str, model: str = "gpt-4o-mini"):
        from openai import OpenAI
        self.client = OpenAI(api_key=api_key)
        self.model = model

    def score_and_extract_batch(
        self,
        resumes: List[Dict[str, str]],
        role_desc: str,
        criteria: List[Criterion],
        job_title: str = ""
    ) -> List[Dict[str, object]]:
        """
        –û–¥–∏–Ω –≤—ã–∑–æ–≤ –Ω–∞ –ø–∞—Ä—Ç–∏—é –¥–æ 5 —Ä–µ–∑—é–º–µ.
        –í—Ö–æ–¥: resumes = [{id: str, text: str}, ...]
        –í—ã—Ö–æ–¥: [
          {
            "id": str,
            "full_name": str,
            "specialization_main": str,
            "specialization_alt": [str],
            "emails": [str],
            "phones": [str],
            "scores": {criterion: float},
            "reasoning": {criterion: str}
          }, ...
        ]
        """
        system = (
            "–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç HR. –î–ª—è –ö–ê–ñ–î–û–ì–û —Ä–µ–∑—é–º–µ –∏–∑ —Å–ø–∏—Å–∫–∞:\n"
            "1) –ò–∑–≤–ª–µ–∫–∏ –ø–æ–ª–Ω–æ–µ –∏–º—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ (–§–ò–û) –∫–∞–∫ 'full_name' (–µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –ø—É—Å—Ç–æ).\n"
            "2) –ò–∑–≤–ª–µ–∫–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é: 'specialization_main' (–¥–æ 80 —Å–∏–º–≤–æ–ª–æ–≤) –∏ –¥–æ 3 –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤.\n"
            "3) –ò–∑–≤–ª–µ–∫–∏ –∫–æ–Ω—Ç–∞–∫—Ç—ã: 'emails' –∏ 'phones' (–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ –≤ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ).\n"
            "4) –û—Ü–µ–Ω–∏ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º 0..5 –∏ –¥–∞–π –∫—Ä–∞—Ç–∫–∏–µ, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è –ø–æ –∫–∞–∂–¥–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é.\n"
            "–í–æ–∑–≤—Ä–∞—â–∞–π JSON —Å—Ç—Ä–æ–≥–æ –ø–æ —Å—Ö–µ–º–µ –¥–ª—è –≤—Å–µ—Ö –≤—Ö–æ–¥–æ–≤."
        )
        payload = {
            "role_title": job_title,
            "role_description": role_desc,
            "criteria": [c.model_dump() for c in criteria],
            "resumes": [{"id": r["id"], "text": r["text"][:18000]} for r in resumes][:5]
        }

        schema = {
            "type": "object",
            "properties": {
                "results": {
                    "type": "array",
                    "maxItems": 5,
                    "items": {
                        "type": "object",
                        "properties": {
                            "id": {"type": "string"},
                            "full_name": {"type": "string"},
                            "specialization_main": {"type": "string"},
                            "specialization_alt": {"type": "array", "items": {"type": "string"}},
                            "emails": {"type": "array", "items": {"type": "string"}},
                            "phones": {"type": "array", "items": {"type": "string"}},
                            "scores": {"type": "object", "additionalProperties": {"type": "number"}},
                            "reasoning": {"type": "object", "additionalProperties": {"type": "string"}}
                        },
                        "required": [
                            "id","full_name","specialization_main","emails","phones","scores"
                        ],
                        "additionalProperties": False
                    }
                }
            },
            "required": ["results"],
            "additionalProperties": False
        }

        resp = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": json.dumps(payload, ensure_ascii=False)}
            ],
            response_format={"type": "json_schema", "json_schema": {"name": "ResumeBatch", "schema": schema}},
            temperature=0.1,
        )
        content = resp.choices[0].message.content
        data = json.loads(content)
        return data.get("results", [])

# ---------- Similarity ----------
def max_similarities(texts: List[str], names: List[str]) -> Tuple[List[float], List[str], pd.DataFrame]:
    n = len(texts)
    norm = [normalize_for_sim(t) for t in texts]
    max_sim = [0.0] * n
    near_name = [""] * n
    pairs = []
    for i in range(n):
        for j in range(i+1, n):
            s = float(fuzz.token_set_ratio(norm[i], norm[j]))
            pairs.append((names[i], names[j], s))
            if s > max_sim[i]:
                max_sim[i] = s; near_name[i] = names[j]
            if s > max_sim[j]:
                max_sim[j] = s; near_name[j] = names[i]
    pairs_df = pd.DataFrame(pairs, columns=["FileA","FileB","Similarity"]).sort_values("Similarity", ascending=False)
    return max_sim, near_name, pairs_df

# ---------- Screening / Scoring ----------
def compute_scores_table(base_rows: List[Dict], criteria: List[Criterion], dup_threshold: int) -> pd.DataFrame:
    df = pd.DataFrame(base_rows)

    # Coverage –∏ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    crit_cols = [f"{c.name} (0-5)" for c in criteria if f"{c.name} (0-5)" in df.columns]
    if crit_cols:
        df["Coverage"] = (df[crit_cols] > 0.0).sum(axis=1) / max(1, len(crit_cols))
        pct = df[crit_cols].rank(pct=True)
        pct.columns = [c.replace(" (0-5)", "::Pct") for c in pct.columns]
        df = pd.concat([df, pct], axis=1)
    else:
        df["Coverage"] = 0.0

    # –í–µ—Å–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
    weights = {c.name: float(c.weight) for c in criteria}
    sumw = sum(weights.values()) or 1.0

    # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª–µ–π
    w_sum = np.zeros(len(df))
    for c in criteria:
        pcol = f"{c.name}::Pct"
        if pcol in df:
            w_sum += weights[c.name] * df[pcol].fillna(0).to_numpy()

    # –ò—Ç–æ–≥: 0.75 –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª–∏ + 0.25 –ø–æ–∫—Ä—ã—Ç–∏–µ (–±–µ–∑ RecencyBoost)
    df["CompositeScore"] = 100.0 * (0.75 * (w_sum / sumw) + 0.25 * df["Coverage"])

    def bucket(x): return "A" if x>=80 else ("B" if x>=60 else "C")
    df["PriorityBucket"] = df["CompositeScore"].apply(bucket)

    # –†–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
    def clamp_local(s: str, n: int) -> str:
        s = re.sub(r"\s+", " ", (s or "").strip())
        return (s[:n-1] + "‚Ä¶") if len(s) > n else s

    def calc_comment(row):
        fio = clamp_local(row.get("–§–ò–û", ""), 80)
        spec = clamp_local(row.get("–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è", ""), 100)
        cov = row.get("Coverage", 0.0)

        pct_cols = [(k.replace("::Pct",""), k) for k in df.columns if k.endswith("::Pct")]
        top = sorted([(crit, float(row[pcol])) for crit, pcol in pct_cols], key=lambda x: x[1], reverse=True)[:3]

        reason = row.get("_Reasoning", {}) or {}
        strengths_parts, examples_parts = [], []
        for crit, _ in top:
            score_val = row.get(f"{crit} (0-5)", 0.0)
            rtxt = clamp_local(str(reason.get(crit, "")), 280)
            if rtxt:
                examples_parts.append(f"{crit} ({score_val:.1f}): {rtxt}")
            strengths_parts.append(f"{crit} ({score_val:.1f})")

        gaps = []
        for c in [c.name for c in criteria]:
            sc = float(row.get(f"{c} (0-5)", 0.0))
            if sc <= 1.5:
                gap_reason = clamp_local(str(reason.get(c, "")), 160)
                gaps.append(f"{c} ({sc:.1f})" + (f": {gap_reason}" if gap_reason else ""))

        risks = []
        if float(row.get("SimilarityMax", 0)) >= dup_threshold: risks.append("—Ä–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–∞ –ø–æ Similarity")
        if not row.get("Email"): risks.append("–Ω–µ—Ç email")
        if not row.get("Phone"): risks.append("–Ω–µ—Ç —Ç–µ–ª–µ—Ñ–æ–Ω–∞")

        strengths_txt = ", ".join(strengths_parts) if strengths_parts else "–Ω–µ—Ç —è–≤–Ω—ã—Ö —Å–∏–ª—å–Ω—ã—Ö —Å—Ç–æ—Ä–æ–Ω"
        examples_txt = " | ".join(examples_parts) if examples_parts else ""
        gaps_txt = "; ".join(gaps) if gaps else "‚Äî"

        return (
            f"{('–ö–∞–Ω–¥–∏–¥–∞—Ç: ' + fio + '. ') if fio else ''}"
            f"{('–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: ' + spec + '. ') if spec else ''}"
            f"–ò—Ç–æ–≥ {row['CompositeScore']:.0f}/100. –ü–æ–∫—Ä—ã—Ç–∏–µ {cov:.0%}. "
            f"–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã: {strengths_txt}. "
            f"{('–ü—Ä–∏–º–µ—Ä—ã: ' + examples_txt + '. ') if examples_txt else ''}"
            f"–ü—Ä–æ–±–µ–ª—ã: {gaps_txt}."
            f"{(' –†–∏—Å–∫–∏: ' + ', '.join(risks) + '.') if risks else ''}"
        )

    df["CalcComment"] = df.apply(calc_comment, axis=1)
    return df

# ---------- Checkpoint ----------
def load_checkpoint(cp_path: str) -> Dict[str, Dict]:
    if not cp_path or not os.path.exists(cp_path): return {}
    cache: Dict[str, Dict] = {}
    with open(cp_path, "r", encoding="utf-8") as f:
        for line in f:
            line=line.strip()
            if not line: continue
            try:
                obj = json.loads(line)
                cache[obj["hash"]] = obj["data"]
            except Exception:
                continue
    return cache

def append_checkpoint(cp_path: str, file_hash: str, data: Dict):
    with open(cp_path, "a", encoding="utf-8") as f:
        f.write(json.dumps({"hash": file_hash, "data": data}, ensure_ascii=False) + "\n")

def reset_checkpoint(cp_path: str):
    if cp_path and os.path.exists(cp_path): os.remove(cp_path)

# ---------- UI ----------
st.set_page_config(page_title="üóëÔ∏è clean_the_garbage.exe ‚Äî Lola, Liza & Partners LLC", layout="wide")
st.title("üóëÔ∏è clean_the_garbage.exe")
st.caption("Lola, Liza & Partners LLC ‚Äî serious screening for massive resume batches")

with st.sidebar:
    st.header("‚öôÔ∏è LLM")
    api_key = st.text_input("OpenAI API Key", type="password")
    model_name = st.selectbox("–ú–æ–¥–µ–ª—å", ["gpt-4o-mini","gpt-4o","gpt-4.1-mini"], index=0)

    st.header("üìå –†–æ–ª—å/–∫—Ä–∏—Ç–µ—Ä–∏–∏")
    job_title = st.text_input("–ù–∞–∑–≤–∞–Ω–∏–µ —Ä–æ–ª–∏ (–æ–ø—Ü.)", value="")
    role_desc = st.text_area("–û–ø–∏—Å–∞–Ω–∏–µ —Ä–æ–ª–∏/–≤–∞–∫–∞–Ω—Å–∏–∏", height=120)

    st.subheader("–ö—Ä–∏—Ç–µ—Ä–∏–∏ (JSON)")
    default_criteria = [
        {"name": "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –æ–ø—ã—Ç—É", "weight": 2.0, "keywords": []},
        {"name": "–î–æ—Å—Ç–∏–∂–µ–Ω–∏—è/—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã", "weight": 1.6, "keywords": []},
        {"name": "–ù–∞–≤—ã–∫–∏ –ø–æ —Ä–æ–ª–∏", "weight": 1.8, "keywords": []},
        {"name": "–ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è/–ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã", "weight": 1.2, "keywords": []},
        {"name": "–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ/—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã", "weight": 0.8, "keywords": []},
    ]
    crit_json = st.text_area("–°–ø–∏—Å–æ–∫ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤", value=json.dumps(default_criteria, ensure_ascii=False, indent=2), height=220)
    criteria: List[Criterion] = []
    try:
        criteria = [Criterion(**c) for c in json.loads(crit_json)]
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {e}")

    st.subheader("–î—É–±–ª–∏–∫–∞—Ç—ã")
    dup_threshold = st.slider("–ü–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ (—Ä–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–∞)", min_value=70, max_value=100, value=90, step=1)

    st.subheader("–í—ã–≤–æ–¥")
    save_path = st.text_input("–ü—É—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è XLSX (–Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ)", value="resume_ranking.xlsx")
    add_pairs = st.checkbox("–õ–∏—Å—Ç SimilarityPairs (—Ç–æ–ø-200)", value=True)

    st.subheader("–ß–µ–∫–ø–æ–∏–Ω—Ç")
    cp_path = st.text_input("–§–∞–π–ª —á–µ–∫–ø–æ–∏–Ω—Ç–∞ (.jsonl)", value="resume_ranker_checkpoint.jsonl")
    colA, colB = st.columns(2)
    with colA:
        resume_from_cp = st.checkbox("–í–æ–∑–æ–±–Ω–æ–≤–ª—è—Ç—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞", value=True)
    with colB:
        if st.button("‚ôªÔ∏è –°–±—Ä–æ—Å–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç"):
            reset_checkpoint(cp_path); st.success("–ß–µ–∫–ø–æ–∏–Ω—Ç —É–¥–∞–ª—ë–Ω.")

st.markdown("## üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—é–º–µ")
files = st.file_uploader("–§–∞–π–ª—ã (PDF/DOCX/TXT/MD/RTF)", type=[ext[1:] for ext in ALLOWED_EXT], accept_multiple_files=True)
run = st.button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏ –≤—ã–≥—Ä—É–∑–∏—Ç—å XLSX")

# ---------- Main ----------
if run:
    if not api_key: st.error("–£–∫–∞–∂–∏—Ç–µ OpenAI API Key"); st.stop()
    if not criteria: st.error("–ó–∞–¥–∞–π—Ç–µ –≤–∞–ª–∏–¥–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏"); st.stop()
    if not files: st.error("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª—ã"); st.stop()

    client = OpenAIClientWrapper(api_key=api_key, model=model_name)
    cache = load_checkpoint(cp_path) if resume_from_cp else {}

    rows: List[Dict] = []
    texts: List[str] = []
    filenames: List[str] = []
    seen_file_hash, seen_text_hash = set(), set()

    status = st.empty()
    st.markdown("#### –ü—Ä–æ–≥—Ä–µ—Å—Å")
    progress_bar = st.progress(0.0)  # –µ–¥–∏–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä

    # -------- Pass 1: –ª–æ–∫–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥/–¥–µ–¥—É–ø --------
    parsed_items = []  # [{id, fh, th, name, text}]
    total_files = len(files)
    for i, f in enumerate(files, start=1):
        status.text(f"–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {f.name} ({i}/{total_files})")
        b = f.getvalue()
        fh = sha1_bytes(b)
        if fh in seen_file_hash:
            progress_bar.progress(i/total_files)
            continue

        try:
            text = read_file_text(f.name, b)
        except Exception as e:
            st.error(f"{f.name}: –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è ‚Äî {e}")
            progress_bar.progress(i/total_files)
            continue

        th = sha1_text(normalize_for_sim(text))
        if th in seen_text_hash:
            progress_bar.progress(i/total_files)
            continue

        parsed_items.append({"id": fh, "fh": fh, "th": th, "name": f.name, "text": text})
        seen_file_hash.add(fh); seen_text_hash.add(th)
        progress_bar.progress(i/total_files)

    kept_after_parse = len(parsed_items)
    status.text(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω. –§–∞–π–ª–æ–≤: {total_files}. –ü–æ—Å–ª–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {kept_after_parse}.")

    if not parsed_items:
        st.warning("–ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞/–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"); st.stop()

    # -------- –°–±—Ä–æ—Å –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø–µ—Ä–µ–¥ –±–∞—Ç—á–∞–º–∏ --------
    progress_bar.progress(0.0)

    # -------- Pass 2: LLM –±–∞—Ç—á–∏ (–¥–æ 5 —Ä–µ–∑—é–º–µ –Ω–∞ –∑–∞–ø—Ä–æ—Å) --------
    batches = list(chunks(parsed_items, 5))
    num_batches = len(batches)
    for bi, batch in enumerate(batches, start=1):
        status.text(f"LLM –±–∞—Ç—á {bi}/{num_batches}: {batch[0]['name']} (+{len(batch)-1} –µ—â—ë)")
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
        need_call = False
        batch_payload = []
        for it in batch:
            if "pack" not in cache.get(it["fh"], {}):
                need_call = True
                batch_payload.append({"id": it["fh"], "text": it["text"]})

        if need_call and batch_payload:
            try:
                packs = client.score_and_extract_batch(
                    resumes=batch_payload,
                    role_desc=role_desc or job_title,
                    criteria=criteria,
                    job_title=job_title
                )
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ LLM –Ω–∞ –±–∞—Ç—á–µ {bi}/{num_batches}: {e}")
                packs = []

            by_id = {p["id"]: p for p in packs}
            for it in batch:
                obj = cache.get(it["fh"], {})
                if it["fh"] in by_id:
                    obj["pack"] = by_id[it["fh"]]
                else:
                    obj["pack"] = {
                        "id": it["fh"],
                        "full_name": "",
                        "specialization_main": "",
                        "specialization_alt": [],
                        "emails": [],
                        "phones": [],
                        "scores": {},
                        "reasoning": {}
                    }
                cache[it["fh"]] = obj
                append_checkpoint(cp_path, it["fh"], obj)

        # –°–±–æ—Ä —Å—Ç—Ä–æ–∫ –¥–ª—è –≤—ã–≤–æ–¥–∞ (—Å —Ñ–æ–ª–±—ç–∫–∞–º–∏)
        for it in batch:
            cobj = cache[it["fh"]]["pack"]
            text = it["text"]

            emails_all = list(cobj.get("emails") or []) or EMAIL_RE.findall(text)
            phones_all = list(cobj.get("phones") or []) or PHONE_CAND_RE.findall(text)
            email_final = emails_all[0] if emails_all else ""
            phone_final = best_phone(phones_all)

            fio = (cobj.get("full_name") or "").strip() or guess_fio(text)

            specialization = (cobj.get("specialization_main") or "").strip()
            if not specialization:
                lines = [l.strip() for l in text.splitlines() if l.strip()][:20]
                for l in lines[:15]:
                    if "@" in l or re.search(r"https?://", l): continue
                    if EMAIL_RE.search(l) or PHONE_CAND_RE.search(l): continue
                    if len(l) <= 140 and (l.istitle() or re.search(r"[A-Za-z–ê-–Ø–∞-—è ]{6,}", l)):
                        specialization = l; break

            score_map = {k: float(cobj.get("scores", {}).get(k, 0.0)) for k in [c.name for c in criteria]}
            base = {
                "–§–∞–π–ª": it["name"],
                "–§–ò–û": fio,
                "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è": specialization,
                "Email": email_final,
                "Phone": phone_final,
                "FullText": text,
                "_FileHash": it["fh"], "_TextHash": it["th"],
                "_Reasoning": cobj.get("reasoning", {}),
            }
            for c in criteria:
                base[f"{c.name} (0-5)"] = score_map.get(c.name, 0.0)

            rows.append(base)
            texts.append(text); filenames.append(it["name"])

        # –æ–±–Ω–æ–≤–ª—è–µ–º –µ–¥–∏–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –ø–æ –±–∞—Ç—á–∞–º
        progress_bar.progress(bi/num_batches)

    if not rows:
        st.warning("–ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ LLM"); st.stop()

    # -------- Pass 3: similarity + email/phone duplicate removal --------
    sim_max, sim_near, pairs_df = max_similarities(texts, filenames)
    for idx, row in enumerate(rows):
        row["SimilarityMax"] = round(sim_max[idx], 1)
        row["NearDuplicateOf"] = sim_near[idx]

    by_email: Dict[str,int] = {}
    by_phone: Dict[str,int] = {}
    keep_mask = [True]*len(rows)
    def norm_email(e: str) -> str: return (e or "").strip().lower()
    def norm_phone(p: str) -> str: return "".join(d for d in (p or "") if d.isdigit())

    for i,r in enumerate(rows):
        e = norm_email(r.get("Email",""))
        if e:
            if e in by_email: keep_mask[i] = False
            else: by_email[e] = i
    for i,r in enumerate(rows):
        if not keep_mask[i]: continue
        p = norm_phone(r.get("Phone",""))
        if p:
            if p in by_phone: keep_mask[i] = False
            else: by_phone[p] = i

    if not pairs_df.empty:
        name_to_idx = {rows[i]["–§–∞–π–ª"]: i for i in range(len(rows))}
        for _, r in pairs_df.iterrows():
            a, b, s = r["FileA"], r["FileB"], float(r["Similarity"])
            ia, ib = name_to_idx.get(a), name_to_idx.get(b)
            if ia is None or ib is None: continue
            if not keep_mask[ia] or not keep_mask[ib]: continue
            if s >= 100.0 or s >= dup_threshold:
                drop = ib if ia < ib else ia
                keep_mask[drop] = False

    rows = [r for i,r in enumerate(rows) if keep_mask[i]]

    # -------- –¢–∞–±–ª–∏—Ü—ã –∏ –≤—ã–≤–æ–¥ --------
    df = compute_scores_table(rows, criteria, dup_threshold)

    show_cols = [
        "–§–∞–π–ª","–§–ò–û","–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è","Email","Phone",
        "Coverage","CompositeScore","PriorityBucket","SimilarityMax","NearDuplicateOf","CalcComment"
    ]
    crit_cols = [f"{c.name} (0-5)" for c in criteria if f"{c.name} (0-5)" in df.columns]
    final_df = df[[c for c in show_cols if c in df.columns] + crit_cols].copy()
    final_df.insert(0, "Rank", range(1, len(final_df)+1))
    final_df = final_df.sort_values(["CompositeScore","SimilarityMax"], ascending=[False, False]).reset_index(drop=True)
    final_df["Rank"] = range(1, len(final_df)+1)

    # –õ–∏—Å—Ç—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats = []
    for c in criteria:
        pcol = f"{c.name}::Pct"
        if pcol in df:
            s = df[pcol].dropna()
            stats.append({
                "Criterion": c.name, "Weight": float(c.weight),
                "MedianPct": float(s.median()) if len(s) else 0.0,
                "P10": float(s.quantile(0.10)) if len(s) else 0.0,
                "P90": float(s.quantile(0.90)) if len(s) else 0.0,
                "CoverageShare": float((df[f"{c.name} (0-5)"]>0).mean()) if f"{c.name} (0-5)" in df else 0.0
            })
    crit_stats_df = pd.DataFrame(stats)
    similarity_pairs_df = pairs_df.head(200).copy() if add_pairs and not pairs_df.empty else pd.DataFrame(columns=["FileA","FileB","Similarity"])

    logic_text = (
        "–ö–∞–∫ —Å—á–∏—Ç–∞–ª–æ—Å—å:\n"
        "‚Ä¢ LLM –≤—ã—Å—Ç–∞–≤–ª—è–µ—Ç –±–∞–ª–ª—ã 0‚Äì5 –ø–æ –≤–∞—à–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∏ –¥–∞—ë—Ç –ø–æ—è—Å–Ω–µ–Ω–∏—è.\n"
        "‚Ä¢ –ö–æ–º–ø–æ–∑–∏—Ç = 0.75√ó–≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª–∏ + 0.25√ó–ø–æ–∫—Ä—ã—Ç–∏–µ.\n"
        "‚Ä¢ –î—É–±–ª–∏–∫–∞—Ç—ã: –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Ñ–∞–π–ª—ã/—Ç–µ–∫—Å—Ç—ã, –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ email/—Ç–µ–ª–µ—Ñ–æ–Ω—ã —É–¥–∞–ª—è—é—Ç—Å—è; Similarity ‚â• –ø–æ—Ä–æ–≥–∞ –ø–æ–º–µ—á–∞–µ—Ç—Å—è –∫–∞–∫ —Ä–∏—Å–∫.\n"
        "‚Ä¢ –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å–æ–¥–µ—Ä–∂–∏—Ç –§–ò–û/—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é, —Ç–æ–ø-3 —Å–∏–ª—å–Ω—ã—Ö —Å –≤—ã–¥–µ—Ä–∂–∫–∞–º–∏, –ø—Ä–æ–±–µ–ª—ã (–Ω–∏–∑–∫–∏–µ –±–∞–ª–ª—ã) –∏ —Ä–∏—Å–∫–∏."
    )
    config_df = pd.DataFrame({
        "Key":[
            "Model","JobTitle","DuplicateThreshold","CheckpointFile",
            "TotalUploaded","KeptAfterLocalDedup","BatchSize","NumBatches","HumanLogic"
        ],
        "Value":[
            model_name, job_title, str(dup_threshold), cp_path,
            str(len(files)), str(len(parsed_items)), "5", str(len(list(chunks(parsed_items, 5)))), logic_text
        ]
    })

    # Write XLSX with formatting
    from openpyxl.styles import PatternFill, Font, Alignment, Border, Side, Color
    from openpyxl.formatting.rule import ColorScaleRule, DataBarRule
    from openpyxl.utils import get_column_letter

    DATA_BAR_COLOR = Color("FF63BE7B")  # –∑–µ–ª—ë–Ω—ã–π ARGB

    xlsx_buf = io.BytesIO()
    with pd.ExcelWriter(xlsx_buf, engine="openpyxl") as xw:
        final_df.to_excel(xw, sheet_name="Ranking", index=False)
        crit_stats_df.to_excel(xw, sheet_name="CriteriaStats", index=False)
        similarity_pairs_df.to_excel(xw, sheet_name="SimilarityPairs", index=False)
        config_df.to_excel(xw, sheet_name="Config", index=False)

        wb = xw.book
        ws = wb["Ranking"]

        # Bold headers + borders
        thin = Side(border_style="thin", color="DDDDDD")
        border_all = Border(left=thin, right=thin, top=thin, bottom=thin)
        for cell in ws[1]:
            cell.font = Font(bold=True)
            cell.border = border_all
            cell.alignment = Alignment(vertical="center", wrap_text=True)
        for r in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
            for cell in r:
                cell.border = border_all
                cell.alignment = Alignment(vertical="top", wrap_text=True)

        # Freeze header + autofilter
        ws.freeze_panes = "A2"
        ws.auto_filter.ref = ws.dimensions

        # Zebra stripes (light)
        stripe = PatternFill(start_color="F7F7F7", end_color="F7F7F7", fill_type="solid")
        for row in range(2, ws.max_row+1, 2):
            for col in range(1, ws.max_column+1):
                ws.cell(row=row, column=col).fill = stripe

        # Column widths
        for col_idx in range(1, ws.max_column+1):
            col = get_column_letter(col_idx)
            max_len = max(len(str(ws[f"{col}{r}"].value)) if ws[f"{col}{r}"].value is not None else 0 for r in range(1, ws.max_row+1))
            ws.column_dimensions[col].width = min(50, max(12, max_len + 2))

        headers = {cell.value: cell.col_idx for cell in ws[1]}
        def col_letter(name: str) -> str: return get_column_letter(headers[name])

        # CompositeScore color scale
        if "CompositeScore" in headers:
            c = col_letter("CompositeScore")
            ws.conditional_formatting.add(
                f"{c}2:{c}{ws.max_row}",
                ColorScaleRule(start_type="min", start_color="F8696B",
                               mid_type="percentile", mid_value=50, mid_color="FFEB84",
                               end_type="max", end_color="63BE7B")
            )

        # Data bars for criteria
        for h, idx in headers.items():
            if h.endswith(" (0-5)"):
                col = get_column_letter(idx)
                rule = DataBarRule(
                    start_type="num", start_value=0,
                    end_type="num", end_value=5,
                    color=DATA_BAR_COLOR,
                    showValue=True
                )
                ws.conditional_formatting.add(f"{col}2:{col}{ws.max_row}", rule)

        # Row highlight by PriorityBucket
        if "PriorityBucket" in headers:
            bc = col_letter("PriorityBucket")
            for row in range(2, ws.max_row+1):
                v = str(ws[f"{bc}{row}"].value or "")
                fill = None
                if v == "A": fill = PatternFill(start_color="E6F4EA", end_color="E6F4EA", fill_type="solid")
                elif v == "B": fill = PatternFill(start_color="FFF5CC", end_color="FFF5CC", fill_type="solid")
                elif v == "C": fill = PatternFill(start_color="FDE7E9", end_color="FDE7E9", fill_type="solid")
                if fill:
                    for col in range(1, ws.max_column+1):
                        ws.cell(row=row, column=col).fill = fill

        # Risk highlight: SimilarityMax >= dup_threshold
        if "SimilarityMax" in headers:
            sc = col_letter("SimilarityMax")
            for row in range(2, ws.max_row+1):
                try:
                    val = float(ws[f"{sc}{row}"].value)
                    if val >= dup_threshold:
                        fill = PatternFill(start_color="FFE8AA", end_color="FFE8AA", fill_type="solid")
                        for col in range(1, ws.max_column+1):
                            ws.cell(row=row, column=col).fill = fill
                        ws[f"{sc}{row}"].font = Font(bold=True)
                except Exception:
                    pass

    data = xlsx_buf.getvalue()

    # Save to server
    try:
        if save_path:
            with open(save_path, "wb") as f: f.write(data)
            st.success(f"–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {save_path}")
    except Exception as e:
        st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ: {e}")

    # Download
    st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å XLSX", data=data,
                       file_name=os.path.basename(save_path) or "resume_ranking.xlsx",
                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

# Footer branding
st.markdown("---")
st.caption("¬© shlukha, Liza & Partners LLC ‚Äî üóëÔ∏è clean_the_garbage.exe")
